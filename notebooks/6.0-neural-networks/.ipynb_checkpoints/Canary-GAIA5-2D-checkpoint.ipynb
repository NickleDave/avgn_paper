{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T19:04:31.109687Z",
     "start_time": "2019-10-26T19:04:31.075512Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T19:04:31.164373Z",
     "start_time": "2019-10-26T19:04:31.111591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T19:04:45.243876Z",
     "start_time": "2019-10-26T19:04:31.168913Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "from avgn.utils.paths import DATA_DIR, most_recent_subdirectory, ensure_dir\n",
    "from avgn.tensorflow.data import _parse_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:29.954Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.autonotebook import tqdm\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import pandas as pd\n",
    "\n",
    "# the nightly build of tensorflow_probability is required as of the time of writing this \n",
    "import tensorflow_probability as tfp\n",
    "ds = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:29.956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1 0.7.0-dev20190510 /mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tensorflow/__init__.py\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__, tfp.__version__, tf.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:29.957Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_SIZE=482608\n",
    "BATCH_SIZE=512\n",
    "TEST_SIZE=10000\n",
    "DIMS = (32, 24, 1)\n",
    "N_TRAIN_BATCHES =int((TRAIN_SIZE-TEST_SIZE)/BATCH_SIZE)\n",
    "N_TEST_BATCHES = int(TEST_SIZE/BATCH_SIZE)\n",
    "TRAIN_BUF = 1000\n",
    "TEST_BUF = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:29.959Z"
    }
   },
   "outputs": [],
   "source": [
    "network_type = 'AE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:29.960Z"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_ID = 'canary_segmented'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:29.962Z"
    }
   },
   "outputs": [],
   "source": [
    "record_loc = DATA_DIR / 'tfrecords' / \"canary_32x24.tfrecord\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:29.963Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the dataset\n",
    "raw_dataset = tf.data.TFRecordDataset([record_loc.as_posix()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:29.967Z"
    }
   },
   "outputs": [],
   "source": [
    "data_types = {\n",
    "    \"spectrogram\": tf.uint8,\n",
    "    \"index\": tf.int64,\n",
    "    \"phrase\": tf.int64,\n",
    "    \"indv\": tf.string,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:29.969Z"
    }
   },
   "outputs": [],
   "source": [
    "# parse each data type to the raw dataset\n",
    "dataset = raw_dataset.map(lambda x: _parse_function(x, data_types=data_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:29.971Z"
    }
   },
   "outputs": [],
   "source": [
    "spec, index, phrase, indv  = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:29.972Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f8430125748>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x384 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(spec.numpy().reshape(DIMS).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:29.974Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = dataset.take(TEST_SIZE).shuffle(TEST_BUF).batch(BATCH_SIZE)\n",
    "train_dataset = dataset.skip(TEST_SIZE).take(TRAIN_SIZE-TEST_SIZE).shuffle(TEST_BUF).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:29.975Z"
    }
   },
   "outputs": [],
   "source": [
    "N_Z = 2\n",
    "\n",
    "def unet_convblock_down(\n",
    "    _input,\n",
    "    channels=16,\n",
    "    kernel=(3, 3),\n",
    "    activation=tf.nn.leaky_relu,\n",
    "    pool_size=(2, 2),\n",
    "    kernel_initializer=\"he_normal\",\n",
    "):\n",
    "    \"\"\" An upsampling convolutional block for a UNET\n",
    "    \"\"\"\n",
    "    conv = tf.keras.layers.Conv2D(\n",
    "        channels,\n",
    "        kernel,\n",
    "        activation=activation,\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=kernel_initializer,\n",
    "    )(_input)\n",
    "    conv = tf.keras.layers.Conv2D(\n",
    "        channels,\n",
    "        kernel,\n",
    "        activation=activation,\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=kernel_initializer,\n",
    "    )(conv)\n",
    "    pool = tf.keras.layers.MaxPooling2D(pool_size=pool_size)(conv)\n",
    "    return conv, pool\n",
    "\n",
    "\n",
    "def unet_convblock_up(\n",
    "    last_conv,\n",
    "    cross_conv,\n",
    "    channels=16,\n",
    "    kernel=(3, 3),\n",
    "    activation=tf.nn.leaky_relu,\n",
    "    pool_size=(2, 2),\n",
    "    kernel_initializer=\"he_normal\",\n",
    "):\n",
    "    \"\"\" A downsampling convolutional block for a UNET\n",
    "    \"\"\"\n",
    "\n",
    "    up_conv = tf.keras.layers.UpSampling2D(size=(2, 2))(last_conv)\n",
    "    merge = tf.keras.layers.concatenate([up_conv, cross_conv], axis=3)\n",
    "    conv = tf.keras.layers.Conv2D(\n",
    "        channels,\n",
    "        kernel,\n",
    "        activation=activation,\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=kernel_initializer,\n",
    "    )(merge)\n",
    "    conv = tf.keras.layers.Conv2D(\n",
    "        channels,\n",
    "        kernel,\n",
    "        activation=activation,\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=kernel_initializer,\n",
    "    )(conv)\n",
    "    return conv\n",
    "\n",
    "\n",
    "def unet_canary():\n",
    "    \"\"\" the architecture for a UNET specific to MNIST\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.layers.Input(shape=(32, 24, 1))\n",
    "    up_1, pool_1 = unet_convblock_down(inputs, channels=16)\n",
    "    up_2, pool_2 = unet_convblock_down(pool_1, channels=32)\n",
    "    up_3, pool_3 = unet_convblock_down(pool_2, channels=64)\n",
    "    conv_middle = tf.keras.layers.Conv2D(\n",
    "        128, (3, 3), activation=tf.nn.leaky_relu, kernel_initializer=\"he_normal\", padding=\"same\"\n",
    "    )(pool_3)\n",
    "    conv_middle = tf.keras.layers.Conv2D(\n",
    "        128, (3, 3), activation=tf.nn.leaky_relu, kernel_initializer=\"he_normal\", padding=\"same\"\n",
    "    )(conv_middle)\n",
    "    down_3 = unet_convblock_up(conv_middle, up_3, channels=64)\n",
    "    down_2 = unet_convblock_up(down_3, up_2, channels=32)\n",
    "    down_1 = unet_convblock_up(down_2, up_1, channels=16)\n",
    "    outputs = tf.keras.layers.Conv2D(1, (1, 1), activation=\"tanh\")(down_1)\n",
    "    return inputs, outputs\n",
    "\n",
    "\n",
    "encoder = [\n",
    "    tf.keras.layers.InputLayer(input_shape=DIMS),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=32, kernel_size=3, strides=(2, 2), activation=tf.nn.leaky_relu\n",
    "    ),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=64, kernel_size=3, strides=(2, 2), activation=tf.nn.leaky_relu\n",
    "    ),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=128, kernel_size=3, strides=(2, 2), activation=tf.nn.leaky_relu\n",
    "    ),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=N_Z),\n",
    "]\n",
    "\n",
    "decoder = [\n",
    "    tf.keras.layers.Dense(units=4 * 3 * 256, activation=tf.nn.leaky_relu),\n",
    "    tf.keras.layers.Reshape(target_shape=(4, 3, 256)),\n",
    "    tf.keras.layers.Conv2DTranspose(\n",
    "        filters=128, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation=tf.nn.leaky_relu\n",
    "    ),\n",
    "    tf.keras.layers.Conv2DTranspose(\n",
    "        filters=64, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation=tf.nn.leaky_relu\n",
    "    ),\n",
    "    tf.keras.layers.Conv2DTranspose(\n",
    "        filters=32, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation=tf.nn.leaky_relu\n",
    "    ),\n",
    "    tf.keras.layers.Conv2DTranspose(\n",
    "        filters=1, kernel_size=3, strides=(1, 1), padding=\"SAME\", activation=\"tanh\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:29.977Z"
    }
   },
   "outputs": [],
   "source": [
    "from avgn.tensorflow.GAIA5 import GAIA, plot_reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:29.978Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow_probability.python.distributions import Chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:29.980Z"
    }
   },
   "outputs": [],
   "source": [
    "# the unet function \n",
    "gen_optimizer = tf.keras.optimizers.Adam(1e-3, beta_1=0.5)\n",
    "disc_optimizer = tf.keras.optimizers.RMSprop(5e-4)\n",
    "    \n",
    "# model\n",
    "model = GAIA(\n",
    "    enc = encoder,\n",
    "    dec = decoder,\n",
    "    unet_function = unet_canary,\n",
    "    gen_optimizer=gen_optimizer,\n",
    "    disc_optimizer = disc_optimizer,\n",
    "    chsq = Chi2(df=1 / BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:29.984Z"
    }
   },
   "outputs": [],
   "source": [
    "# exampled data for plotting results\n",
    "example_data = next(iter(test_dataset))\n",
    "example_data = (\n",
    "        tf.cast(tf.reshape(example_data[0], [BATCH_SIZE] + list(DIMS)), tf.float32)\n",
    "        / 255\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:29.985Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compute_loss(example_data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:29.987Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_reconstruction(model, example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:29.989Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_losses(losses):\n",
    "    cols = list(losses.columns)\n",
    "    fig, axs = plt.subplots(ncols = len(cols), figsize= (len(cols)*4, 4))\n",
    "    for ci, col in enumerate(cols):\n",
    "        if len(cols) == 1:\n",
    "            ax = axs\n",
    "        else:\n",
    "            ax = axs.flatten()[ci]\n",
    "        ax.plot(losses[col].values)\n",
    "        ax.set_title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:29.990Z"
    }
   },
   "outputs": [],
   "source": [
    "# a pandas dataframe to save the loss information to\n",
    "losses = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"X_D_G_X_loss\",\n",
    "        \"X_D_G_Zi_loss\",\n",
    "        \"X_G_loss\",\n",
    "        \"X_D_X_loss\",\n",
    "        \"G_loss\",\n",
    "        \"D_loss\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:29.992Z"
    }
   },
   "outputs": [],
   "source": [
    "N_TRAIN_BATCHES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:29.997Z"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    # train\n",
    "    for batch, train_x in tqdm(\n",
    "        zip(range(N_TRAIN_BATCHES), train_dataset), total=N_TRAIN_BATCHES\n",
    "    ):\n",
    "        #print(train_x[-1].numpy()[:4])\n",
    "        x = tf.cast(tf.reshape(train_x[0], [BATCH_SIZE] + list(DIMS)), tf.float32) / 255\n",
    "        model.train_net(x)\n",
    "    # test on holdout\n",
    "    loss = []\n",
    "    for batch, test_x in tqdm(\n",
    "        zip(range(N_TEST_BATCHES), test_dataset), total=N_TEST_BATCHES\n",
    "    ):\n",
    "        x = tf.cast(tf.reshape(test_x[0], [BATCH_SIZE] + list(DIMS)), tf.float32) / 255\n",
    "        loss.append(model.compute_loss(x))\n",
    "    losses.loc[len(losses)] = np.mean(loss, axis=0)\n",
    "    # plot results\n",
    "    display.clear_output()\n",
    "    plot_reconstruction(model, example_data, zm = 2)\n",
    "    # \n",
    "    plot_losses(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:29.998Z"
    }
   },
   "outputs": [],
   "source": [
    "z, xg, zi, xi, d_xi, d_x, d_xg = model.network_pass(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:30.000Z"
    }
   },
   "outputs": [],
   "source": [
    "0.006295"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:30.004Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_reconstruction(model, example_data, zm = 2)\n",
    "# \n",
    "plot_losses(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:30.006Z"
    }
   },
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:30.008Z"
    }
   },
   "outputs": [],
   "source": [
    "(losses.X_D_X_loss.values - losses.X_D_G_Zi_loss.values) / losses.X_D_G_Zi_loss.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:30.012Z"
    }
   },
   "outputs": [],
   "source": [
    "losses.D_prop_gen.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:30.014Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_reconstruction(model, example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:30.016Z"
    }
   },
   "outputs": [],
   "source": [
    "losses.d_xg_loss.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:30.017Z"
    }
   },
   "outputs": [],
   "source": [
    "#save_loc = DATA_DIR / 'models' / network_type / DATASET_ID\n",
    "#ensure_dir(save_loc)\n",
    "\n",
    "# Save the entire model to a HDF5 file.\n",
    "# The '.h5' extension indicates that the model shuold be saved to HDF5.\n",
    "#model.save_weights((save_loc / (str(epoch).zfill(4))).as_posix()) \n",
    "\n",
    "# Recreate the exact same model, including its weights and the optimizer\n",
    "#new_model = tf.keras.models.load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot samples from latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:30.019Z"
    }
   },
   "outputs": [],
   "source": [
    "z = model.encode(example_data).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:30.023Z"
    }
   },
   "outputs": [],
   "source": [
    "xmax, ymax = np.max(z, axis=0)\n",
    "xmin, ymin = np.min(z, axis=0)\n",
    "print(xmax, ymax, xmin, ymin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:30.024Z"
    }
   },
   "outputs": [],
   "source": [
    "# sample from grid\n",
    "nx = ny= 30\n",
    "meshgrid = np.meshgrid(np.linspace(xmin, xmax, nx), np.linspace(ymin, ymax, ny))\n",
    "meshgrid = np.array(meshgrid).reshape(2, nx*ny).T\n",
    "x_grid = model.decode(meshgrid)\n",
    "x_grid = x_grid.numpy().reshape(nx, ny, DIMS[0], DIMS[1], DIMS[2])\n",
    "# fill canvas\n",
    "canvas = np.zeros((nx*DIMS[0], ny*DIMS[1]))\n",
    "for xi in range(nx):\n",
    "    for yi in range(ny):\n",
    "        canvas[xi*DIMS[0]:xi*DIMS[0]+DIMS[0], yi*DIMS[1]:yi*DIMS[1]+DIMS[1]] = x_grid[xi, yi,:,:,:].squeeze()\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "ax.matshow(canvas, vmin = 0, cmap=plt.cm.Greys, origin = 'lower')\n",
    "ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  plot dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:30.026Z"
    }
   },
   "outputs": [],
   "source": [
    "all_x = []\n",
    "all_z = []\n",
    "all_indv = []\n",
    "all_labels = []\n",
    "for batch, train_x in tqdm(\n",
    "    zip(range(N_TRAIN_BATCHES), train_dataset), total=N_TRAIN_BATCHES\n",
    "):\n",
    "    x = train_x[0]\n",
    "    all_x.append(x)\n",
    "    x = tf.cast(tf.reshape(x, [BATCH_SIZE] + list(DIMS)), tf.float32) / 255\n",
    "    #model.train_net(x)\n",
    "    all_z.append(model.encode(x).numpy())\n",
    "    all_indv.append(train_x[3].numpy())\n",
    "    all_labels.append(train_x[2].numpy())\n",
    "\n",
    "all_z = np.vstack(all_z)\n",
    "all_indv = np.concatenate(all_indv)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "all_x = np.concatenate(all_x)\n",
    "all_x = np.reshape(all_x, [len(all_x)] + list(DIMS[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:30.028Z"
    }
   },
   "outputs": [],
   "source": [
    "from avgn.visualization.projections import scatter_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:30.029Z"
    }
   },
   "outputs": [],
   "source": [
    "np.shape(all_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:30.031Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(all_z[:,0], all_z[:,1], s=1, c='k', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-26T19:04:30.032Z"
    }
   },
   "outputs": [],
   "source": [
    "scatter_spec(\n",
    "        np.vstack(all_z),\n",
    "        all_x,\n",
    "        column_size=15,\n",
    "        x_range = [-4,4],\n",
    "        y_range = [-4,4],\n",
    "        pal_color=\"hls\",\n",
    "        color_points=False,\n",
    "        enlarge_points=20,\n",
    "        figsize=(10, 10),\n",
    "        scatter_kwargs = {\n",
    "            'labels': list(all_labels),\n",
    "            'alpha':0.25,\n",
    "            's': 1,\n",
    "            'show_legend': False\n",
    "        },\n",
    "        matshow_kwargs = {\n",
    "            'cmap': plt.cm.Greys\n",
    "        },\n",
    "        line_kwargs = {\n",
    "            'lw':1,\n",
    "            'ls':\"solid\",\n",
    "            'alpha':0.25,\n",
    "        },\n",
    "        draw_lines=True\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
