{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to get this working in full, recon loss needs to be applied component needs to be added to discrim reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T19:21:03.599187Z",
     "start_time": "2019-11-06T19:21:03.577521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=2\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T19:21:06.554175Z",
     "start_time": "2019-11-06T19:21:03.600853Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/AD/tsainbur/anaconda3/envs/py19/lib/python3.6/site-packages/tqdm/autonotebook.py:17: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "from avgn.utils.paths import DATA_DIR, most_recent_subdirectory, ensure_dir\n",
    "from avgn.tensorflow.data import _parse_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T19:21:06.865565Z",
     "start_time": "2019-11-06T19:21:06.556170Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.autonotebook import tqdm\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import pandas as pd\n",
    "\n",
    "# the nightly build of tensorflow_probability is required as of the time of writing this \n",
    "import tensorflow_probability as tfp\n",
    "ds = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T19:21:06.916935Z",
     "start_time": "2019-11-06T19:21:06.867481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0 0.8.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__, tfp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T19:21:07.002760Z",
     "start_time": "2019-11-06T19:21:06.919977Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_SIZE=101726\n",
    "BATCH_SIZE=32\n",
    "TEST_SIZE=10000\n",
    "DIMS = (32, 32, 1)\n",
    "N_TRAIN_BATCHES =int((TRAIN_SIZE-TEST_SIZE)/BATCH_SIZE)\n",
    "N_TEST_BATCHES = int(TEST_SIZE/BATCH_SIZE)\n",
    "TRAIN_BUF = 1000\n",
    "TEST_BUF = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T19:21:07.088860Z",
     "start_time": "2019-11-06T19:21:07.005014Z"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_ID = 'starling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T19:21:07.180638Z",
     "start_time": "2019-11-06T19:21:07.090608Z"
    }
   },
   "outputs": [],
   "source": [
    "record_loc = DATA_DIR / 'tfrecords' / \"starling_32.tfrecords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T19:21:07.943031Z",
     "start_time": "2019-11-06T19:21:07.182334Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the dataset\n",
    "raw_dataset = tf.data.TFRecordDataset([record_loc.as_posix()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T19:21:08.005551Z",
     "start_time": "2019-11-06T19:21:07.944697Z"
    }
   },
   "outputs": [],
   "source": [
    "data_types = {\n",
    "    \"spectrogram\": tf.uint8,\n",
    "    \"index\": tf.int64,\n",
    "    \"indv\": tf.string,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T19:21:08.420330Z",
     "start_time": "2019-11-06T19:21:08.007136Z"
    }
   },
   "outputs": [],
   "source": [
    "# parse each data type to the raw dataset\n",
    "dataset = raw_dataset.map(lambda x: _parse_function(x, data_types=data_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T19:21:08.482124Z",
     "start_time": "2019-11-06T19:21:08.421963Z"
    }
   },
   "outputs": [],
   "source": [
    "spec, index, indv  = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T19:21:08.728813Z",
     "start_time": "2019-11-06T19:21:08.483813Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7ef03c5128>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(spec.numpy().reshape(DIMS[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T19:21:08.780708Z",
     "start_time": "2019-11-06T19:21:08.730382Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = dataset.take(TEST_SIZE).shuffle(TRAIN_BUF).batch(BATCH_SIZE)\n",
    "train_dataset = dataset.skip(TEST_SIZE).take(TRAIN_SIZE-TEST_SIZE).shuffle(TEST_BUF).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-06T19:21:03.233Z"
    }
   },
   "outputs": [],
   "source": [
    "# exampled data for plotting results\n",
    "example_data = next(iter(test_dataset))\n",
    "example_data = (\n",
    "        tf.cast(tf.reshape(example_data[0], [BATCH_SIZE] + list(DIMS)), tf.float32)\n",
    "        / 255\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-06T19:21:03.235Z"
    }
   },
   "outputs": [],
   "source": [
    "latent_dim = N_Z = 128\n",
    "\n",
    "encoder = [\n",
    "    tf.keras.layers.InputLayer(input_shape=DIMS),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=32, kernel_size=3, strides=(2, 2), activation=\"relu\"\n",
    "    ),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=64, kernel_size=3, strides=(2, 2), activation=\"relu\"\n",
    "    ),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=128, kernel_size=3, strides=(2, 2), activation=\"relu\"\n",
    "    ),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=N_Z*2),\n",
    "]\n",
    "\n",
    "decoder = [\n",
    "    tf.keras.layers.Dense(units=4 * 4 * 256, activation=\"relu\"),\n",
    "    tf.keras.layers.Reshape(target_shape=(4, 4, 256)),\n",
    "    tf.keras.layers.Conv2DTranspose(\n",
    "        filters=128, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation=\"relu\"\n",
    "    ),\n",
    "    tf.keras.layers.Conv2DTranspose(\n",
    "        filters=64, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation=\"relu\"\n",
    "    ),\n",
    "    tf.keras.layers.Conv2DTranspose(\n",
    "        filters=32, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation=\"relu\"\n",
    "    ),\n",
    "    tf.keras.layers.Conv2DTranspose(\n",
    "        filters=1, kernel_size=3, strides=(1, 1), padding=\"SAME\", activation=None#tf.nn.sigmoid\n",
    "    ),\n",
    "]\n",
    "\n",
    "discrim_project = [\n",
    "    tf.keras.layers.Conv2D(\n",
    "                filters=32, kernel_size=3, strides=(2, 2), activation=tf.nn.relu\n",
    "            ),\n",
    "    tf.keras.layers.Conv2D(\n",
    "                filters=64, kernel_size=3, strides=(2, 2), activation=tf.nn.relu\n",
    "            ),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=64, kernel_size=3, strides=(2, 2), activation=tf.nn.relu\n",
    "    ),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=128, kernel_size=3, strides=(2, 2), activation=None\n",
    "    ),\n",
    "    tf.keras.layers.Flatten()\n",
    "]\n",
    "\n",
    "discrim_decision = [\n",
    "    tf.keras.layers.Dense(units=1, activation = None)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-06T19:21:03.237Z"
    }
   },
   "outputs": [],
   "source": [
    "class VAEGAN(tf.keras.Model):\n",
    "    \"\"\"a VAEGAN class for tensorflow\n",
    "    \n",
    "    Extends:\n",
    "        tf.keras.Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, beta=1.0, **kwargs):\n",
    "        super(VAEGAN, self).__init__()\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.enc = tf.keras.Sequential(self.enc)\n",
    "        self.dec = tf.keras.Sequential(self.dec)\n",
    "        self.disc_project = tf.keras.Sequential(self.discrim_project)\n",
    "        self.disc_decide = tf.keras.Sequential(self.discrim_decision)\n",
    "\n",
    "    def encode(self, x):\n",
    "        mu, sigma = tf.split(self.enc(x), num_or_size_splits=2, axis=1)\n",
    "        return mu, sigma\n",
    "\n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.dec(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * 0.5) + mean\n",
    "\n",
    "    def reconstruct(self, x):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        return self.decode(z, apply_sigmoid=True)\n",
    "\n",
    "    def discriminate_project(self, x, apply_sigmoid=False):\n",
    "        logits = self.disc_project(x)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits\n",
    "\n",
    "    def discriminate_decide(self, x):\n",
    "        return self.disc_decide(x)\n",
    "\n",
    "    def discriminate(self, x):\n",
    "        return self.disc_decide(self.disc_project(x))\n",
    "\n",
    "    @tf.function\n",
    "    def compute_loss(self, x):\n",
    "\n",
    "        #### Autoencoder part\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mean, logvar)\n",
    "        x_logit = self.decode(z)\n",
    "        x_probs = tf.sigmoid(x_logit)\n",
    "\n",
    "        # loss for latent\n",
    "        logpz = log_normal_pdf(z, 0.0, 0.0)\n",
    "        logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "\n",
    "        #### GAN PART\n",
    "        # sample and reconstruct z\n",
    "        z_samp = tf.random.normal(shape=(len(x), z.shape[-1]))\n",
    "        xg_samp = self.decode(z_samp, apply_sigmoid=True)\n",
    "\n",
    "        ### pass through discriminator\n",
    "        # pass generated x through discriminator\n",
    "        d_xg_samp = self.discriminate(xg_samp)\n",
    "        # get latent discrim for x\n",
    "        ld_x = self.discriminate_project(x, apply_sigmoid=True)\n",
    "        # make decision for x\n",
    "        d_x = self.discriminate(x)\n",
    "        # get latent for discrim xg\n",
    "        ld_xg = self.discriminate_project(x_probs)\n",
    "        d_xg = self.discriminate_decide(ld_xg)\n",
    "        \n",
    "        VAEGAN_recon = False\n",
    "        if VAEGAN_recon:\n",
    "            # loss for reconstruction in latent\n",
    "            cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                logits=tf.keras.layers.Flatten()(ld_xg),\n",
    "                labels=tf.keras.layers.Flatten()(ld_x),\n",
    "            )\n",
    "        else:\n",
    "            # loss for reconstruction in x\n",
    "            cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                logits=tf.keras.layers.Flatten()(x_logit),\n",
    "                labels=tf.keras.layers.Flatten()(x),\n",
    "            )\n",
    "\n",
    "        # oveall loss for VAE\n",
    "        logpx_z = -tf.reduce_sum(cross_ent, axis=[1])\n",
    "        #elbo_loss = tf.reduce_mean(cross_ent)\n",
    "        elbo_loss = -tf.reduce_mean(logpx_z + (logpz - logqz_x))\n",
    "\n",
    "        # losses for GAN\n",
    "        disc_real_loss = gan_loss(logits=d_x, is_real=True)\n",
    "        disc_fake_loss = gan_loss(logits=d_xg_samp, is_real=False)\n",
    "        gen_loss = gan_loss(logits=d_xg_samp, is_real=True)\n",
    "        self.D_prop = sigmoid(\n",
    "            disc_fake_loss - gen_loss, shift=0.0, mult=self.sig_mult\n",
    "        )\n",
    "\n",
    "        # just descriptive\n",
    "        latent_loss = - tf.reduce_mean(logpz - logqz_x)\n",
    "        recon_loss = -tf.reduce_mean(logpx_z)\n",
    "\n",
    "        # net losses\n",
    "        #enc_loss = dec_loss = elbo_loss + gen_fake_loss\n",
    "        #disc_loss = (disc_fake_loss + disc_real_loss) * self.D_prop\n",
    "        enc_loss = dec_loss = 1 * gen_loss + 1 * elbo_loss \n",
    "        disc_loss = (disc_fake_loss + disc_real_loss) #  * self.D_prop\n",
    "        \n",
    "        return (\n",
    "            enc_loss,\n",
    "            dec_loss,\n",
    "            disc_loss,\n",
    "            self.D_prop,\n",
    "            latent_loss,\n",
    "            recon_loss,\n",
    "            disc_real_loss,\n",
    "            disc_fake_loss,\n",
    "            gen_loss,\n",
    "        )\n",
    "\n",
    "    def compute_gradients(self, x):\n",
    "        with tf.GradientTape() as enc_tape, tf.GradientTape() as dec_tape, tf.GradientTape() as disc_tape:\n",
    "            losses = self.compute_loss(x)\n",
    "            enc_loss = losses[0]\n",
    "            dec_loss = losses[1]\n",
    "            disc_loss = losses[2]\n",
    "\n",
    "        enc_gradients = enc_tape.gradient(enc_loss, self.enc.trainable_variables+self.dec.trainable_variables)\n",
    "        disc_gradients = disc_tape.gradient(\n",
    "            disc_loss,\n",
    "            self.disc_project.trainable_variables\n",
    "            + self.disc_decide.trainable_variables,\n",
    "        )\n",
    "\n",
    "        return enc_gradients, disc_gradients\n",
    "\n",
    "    def apply_gradients(self, enc_gradients, disc_gradients):\n",
    "        self.enc_optimizer.apply_gradients(\n",
    "            zip(enc_gradients, self.enc.trainable_variables + self.dec.trainable_variables)\n",
    "        )\n",
    "\n",
    "        self.disc_optimizer.apply_gradients(\n",
    "            zip(\n",
    "                disc_gradients,\n",
    "                self.disc_project.trainable_variables\n",
    "                + self.disc_decide.trainable_variables,\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    @tf.function\n",
    "    def train_net(self, x):\n",
    "        enc_gradients, disc_gradients = self.compute_gradients(x)\n",
    "        self.apply_gradients( enc_gradients, disc_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-06T19:21:03.239Z"
    }
   },
   "outputs": [],
   "source": [
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(\n",
    "      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "      axis=raxis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-06T19:21:03.240Z"
    },
    "code_folding": [
     0,
     15
    ]
   },
   "outputs": [],
   "source": [
    "def gan_loss(logits, is_real=True):\n",
    "    \"\"\"Computes standard gan loss between logits and labels\n",
    "                \n",
    "        Arguments:\n",
    "            logits {[type]} -- output of discriminator\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            isreal {bool} -- whether labels should be 0 (fake) or 1 (real) (default: {True})\n",
    "        \"\"\"\n",
    "    if is_real:\n",
    "        labels = tf.ones_like(logits)\n",
    "    else:\n",
    "        labels = tf.zeros_like(logits)\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels, logits))\n",
    "\n",
    "def sigmoid(x, shift=0.0, mult=20):\n",
    "    \"\"\" squashes a value with a sigmoid\n",
    "    \"\"\"\n",
    "    return tf.constant(1.0) / (\n",
    "        tf.constant(1.0) + tf.exp(-tf.constant(1.0) * (x * mult))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-06T19:21:03.242Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_reconstruction(model, example_data, BATCH_SIZE, N_Z, nex=8, zm=2):\n",
    "\n",
    "    example_data_reconstructed = model.reconstruct(example_data)\n",
    "    samples = model.decode(tf.random.normal(shape=(BATCH_SIZE, N_Z)), apply_sigmoid=True)\n",
    "    fig, axs = plt.subplots(ncols=nex, nrows=3, figsize=(zm * nex, zm * 3))\n",
    "    for axi, (dat, lab) in enumerate(\n",
    "        zip(\n",
    "            [example_data, example_data_reconstructed, samples],\n",
    "            [\"data\", \"data recon\", \"samples\"],\n",
    "        )\n",
    "    ):\n",
    "        for ex in range(nex):\n",
    "            axs[axi, ex].matshow(\n",
    "                dat.numpy()[ex].squeeze(), cmap=plt.cm.Greys#, vmin=0, vmax=1\n",
    "            )\n",
    "            axs[axi, ex].axes.get_xaxis().set_ticks([])\n",
    "            axs[axi, ex].axes.get_yaxis().set_ticks([])\n",
    "        axs[axi, 0].set_ylabel(lab)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-06T19:21:03.244Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-06T19:21:03.245Z"
    }
   },
   "outputs": [],
   "source": [
    "# model\n",
    "model = VAEGAN(\n",
    "    enc = encoder,\n",
    "    dec = decoder,\n",
    "    discrim_project=discrim_project, \n",
    "    discrim_decision=discrim_decision,\n",
    "    enc_optimizer = tf.keras.optimizers.Adam(1e-4, beta_1=0.5),\n",
    "    disc_optimizer = tf.keras.optimizers.RMSprop(1e-4),\n",
    "    sig_mult = 10, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-06T19:21:03.246Z"
    }
   },
   "outputs": [],
   "source": [
    "model.train_net(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-06T19:21:03.248Z"
    }
   },
   "outputs": [],
   "source": [
    "enc_gradients, disc_gradients = model.compute_gradients(example_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-06T19:21:03.249Z"
    }
   },
   "outputs": [],
   "source": [
    "np.concatenate([[tf.reduce_sum(i).numpy() for i in grads] for grads in [enc_gradients, disc_gradients]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-06T19:21:03.251Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_reconstruction(model, example_data, BATCH_SIZE, N_Z=N_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-06T19:21:03.252Z"
    }
   },
   "outputs": [],
   "source": [
    "# a pandas dataframe to save the loss information to\n",
    "losses = pd.DataFrame(columns=[\n",
    "    'enc_loss',\n",
    "    'dec_loss',\n",
    "    'disc_loss',\n",
    "    'D_prop',\n",
    "    'latent_loss',\n",
    "    'recon_loss',\n",
    "    'disc_real_loss',\n",
    "    'disc_fake_loss',\n",
    "    'gen_fake_loss',\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-06T19:21:03.254Z"
    }
   },
   "outputs": [],
   "source": [
    "N_TRAIN_BATCHES = 1000\n",
    "N_TEST_BATCHES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-06T19:21:03.255Z"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 5000\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-06T19:21:03.257Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_losses(losses):\n",
    "    cols = list(losses.columns)\n",
    "    fig, axs = plt.subplots(ncols = len(cols), figsize= (len(cols)*4, 4))\n",
    "    for ci, col in enumerate(cols):\n",
    "        if len(cols) == 1:\n",
    "            ax = axs\n",
    "        else:\n",
    "            ax = axs.flatten()[ci]\n",
    "        ax.plot(losses[col].values)\n",
    "        ax.set_title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-06T19:21:03.261Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for epoch in range(epoch, n_epochs):\n",
    "    # train\n",
    "    for batch, train_x in tqdm(\n",
    "        zip(range(N_TRAIN_BATCHES), train_dataset), total=N_TRAIN_BATCHES\n",
    "    ):\n",
    "        x = tf.cast(tf.reshape(train_x[0], [BATCH_SIZE] + list(DIMS)), tf.float32) / 255\n",
    "        model.train_net(x)\n",
    "    # test on holdout\n",
    "    loss = []\n",
    "    for batch, test_x in tqdm(\n",
    "        zip(range(N_TEST_BATCHES), test_dataset), total=N_TEST_BATCHES\n",
    "    ):\n",
    "        x = tf.cast(tf.reshape(test_x[0], [BATCH_SIZE] + list(DIMS)), tf.float32) / 255\n",
    "        loss.append(model.compute_loss(x))\n",
    "    losses.loc[len(losses)] = np.mean(loss, axis=0)\n",
    "    # plot results\n",
    "    display.clear_output()\n",
    "\n",
    "    plot_reconstruction(model, example_data, BATCH_SIZE, N_Z=latent_dim)\n",
    "    \n",
    "    # plot losses\n",
    "    fig, axs = plt.subplots(ncols=4, figsize=(22,4))\n",
    "    # elbo loss vs GAN loss\n",
    "    axs[0].plot(losses.latent_loss + losses.recon_loss, label = 'elbo loss')\n",
    "    axs[0].plot(losses.gen_fake_loss, label = 'gen_fake_loss')\n",
    "    axs[0].legend()\n",
    "    # latent loss vs AE loss\n",
    "    axs[1].plot(losses.latent_loss, label = 'latent_loss')\n",
    "    axs[1].plot(losses.recon_loss, label = 'recon_loss')\n",
    "    axs[1].legend()\n",
    "    # discriminator vs generator loss\n",
    "    axs[2].plot(losses.disc_real_loss, label = 'disc_real_loss')\n",
    "    axs[2].plot(losses.disc_fake_loss, label = 'disc_fake_loss')\n",
    "    axs[2].plot(losses.gen_fake_loss, label = 'gen_fake_loss')\n",
    "    axs[2].legend()\n",
    "\n",
    "    # D prop\n",
    "    axs[3].plot(losses.D_prop, label = 'D_prop')\n",
    "    axs[3].legend()\n",
    "    \n",
    "    # plot Z\n",
    "    mean, logvar = tf.split(model.enc(example_data), num_or_size_splits=2, axis=1)\n",
    "    z = model.reparameterize(mean, logvar).numpy()\n",
    "    fig, axs = plt.subplots(ncols=3, figsize=(18,4))\n",
    "    axs[0].hist(mean.numpy().flatten())\n",
    "    axs[1].hist(logvar.numpy().flatten())\n",
    "    axs[2].hist(z.flatten())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-06T19:21:03.262Z"
    }
   },
   "outputs": [],
   "source": [
    "# balance GAN / VAE so that the VAE tries to autoencode but doesn't take over completely... "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-py19] *",
   "language": "python",
   "name": "conda-env-anaconda3-py19-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
